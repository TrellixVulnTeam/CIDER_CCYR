{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section1: Generate Negative Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd = defaultdict(list)\n",
    "# for o in data:\n",
    "#     context = o['context']\n",
    "#     arg2 = o['qas'][0]['answers'][0]['text']\n",
    "# #     index = context.index(arg2)\n",
    "#     relation = o['qas'][0]['question']\n",
    "#     pos_tags = '_'.join([o.pos_ for o in nlp(arg2)])\n",
    "#     dd[relation].append(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('adversarial-outputs/spanbert-squad/predictions_test.json') as f:\n",
    "    predictions = json.load(f)\n",
    "with ('original_data/relations_only/adversarial_train_and_test.json') as f: \n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = defaultdict(list)\n",
    "for i, ((pred_k, pred_v), d) in \\\n",
    "enumerate(zip(predictions.items(), data)):\n",
    "    if pred_v == '.' or pred_v == '':\n",
    "        continue\n",
    "    ans = d['qas'][0]['answers'][0]['text']\n",
    "    context = d['context']\n",
    "    relation = d['qas'][0]['question']\n",
    "    if ans in context:\n",
    "        continue\n",
    "    dialog_id = ' '.join(pred_k.split('-')[:3])\n",
    "    dd[f'{dialog_id}+{relation}'].append(pred_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dd, open('negative_options.json', 'w'), indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Generate iteration 0 Adversarial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_data = json.load(open('/home/emrys/Data/csk_data/csk_qa_22_01/data/qa/train_and_test.json'))\n",
    "qa_data_old = json.load(open('/home/emrys/Data/csk_data/csk_qa_22_01/data/qa_old/train_and_test.json'))\n",
    "dd_ans = defaultdict(list)\n",
    "for d in qa_data_old:\n",
    "    dialog_id = ' '.join(d['qas'][0]['id'].split('-')[:3])\n",
    "    question = d['qas'][0]['question']\n",
    "    dd_ans[f\"{dialog_id}+{question}\"].append(d['qas'][0]['answers'][0]['text'])\n",
    "\n",
    "\n",
    "samples = []\n",
    "for index, (d, d_old) in enumerate(zip(qa_data, qa_data_old)):\n",
    "    relation = d['qas'][0]['question']\n",
    "    dialog_id = ' '.join(d['qas'][0]['id'].split('-')[:3])\n",
    "    options = dd[f'{dialog_id}+{relation}']\n",
    "    question = d_old['qas'][0]['question']\n",
    "    \n",
    "    correct_answers = dd_ans[f\"{dialog_id}+{question}\"]\n",
    "    options = list(set([o for o in options if o not in correct_answers]))\n",
    "    \n",
    "    if len(options) < 3:\n",
    "        continue\n",
    "    options = random.sample(options, 3)\n",
    "    insert_position = random.randint(0,3)\n",
    "    correct_ans = d['qas'][0]['answers'][0]['text']\n",
    "    options.insert(insert_position, correct_ans)\n",
    "    for i in range(len(options)):\n",
    "        options[i] = '\"' + options[i] + '\"'\n",
    "    sample = \"\"\n",
    "    sample += (f\"{index},{d['qas'][0]['id']},\")\n",
    "    sample += (\"0\" + ',')\n",
    "#     sample += (f'{relation},') # start phrase\n",
    "    sample += (f\"\\\"{d_old['qas'][0]['question']}\\\",\") # start phrase\n",
    "    context = d['context'].replace(\"\\\"\", \"\\'\")\n",
    "    sample += (f\"\\\"{context}\\\",\") # sent1    \n",
    "#     sample += (f\"\\\"{d_old['qas'][0]['question']}\\\",\") # sent2\n",
    "    sample += (f\"{relation},\") # sent2\n",
    "    sample += (f'gen,')\n",
    "    sample += (','.join(options)+',')\n",
    "    sample += (f'{insert_position}\\n')\n",
    "    samples.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(samples, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_iter0.csv', 'w') as f:\n",
    "    f.write(\",video-id,fold-ind,startphrase,sent1,sent2,gold-source,ending0,ending1,ending2,ending3,label\\n\")\n",
    "    for o in test:\n",
    "        f.write(o)\n",
    "\n",
    "with open('train_iter0.csv', 'w') as f:\n",
    "    f.write(\",video-id,fold-ind,startphrase,sent1,sent2,gold-source,ending0,ending1,ending2,ending3,label\\n\")\n",
    "    for o in train:\n",
    "        f.write(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Generate later iterations 2-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 # stands for the 0th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f'../dataset/val_iter{i}.csv')\n",
    "train = pd.read_csv(f'../dataset/train_iter{i}.csv')\n",
    "# this generated file may be in the corresponding model folder\n",
    "replace_index = np.load('../replace_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test)):\n",
    "    rep_i = replace_index[idx]\n",
    "    gold = test.loc[idx, 'label']\n",
    "    if rep_i == gold:\n",
    "        continue\n",
    "    rep_text = test.loc[idx, f'ending{rep_i}']\n",
    "    \n",
    "    dialog_id = ' '.join(test.loc[idx, 'video-id'].split('-')[:3])\n",
    "    relation = test.loc[idx, 'sent2']\n",
    "    options = dd[f'{dialog_id}+{relation}']\n",
    "    \n",
    "    question = test.loc[idx, 'startphrase']\n",
    "    correct_answers = dd_ans[f\"{dialog_id}+{question}\"]\n",
    "    options = list(set([o for o in options if o not in correct_answers]))\n",
    "    \n",
    "    existing_answers = [test.loc[idx, f'ending{i}'] for i in range(4)]\n",
    "    options = [o for o in options if o not in existing_answers]\n",
    "    \n",
    "    if len(options) > 0:\n",
    "        candidate = random.sample(options, 1)\n",
    "        test.loc[idx, f'ending{rep_i}'] = candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test = pd.concat([train, test])\n",
    "train, test = train_test_split(train_and_test, test_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(f'../dataset/train_iter{i+1}.csv', index=False)\n",
    "test.to_csv(f'../dataset/val_iter{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4:Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 35\n",
    "test = pd.read_csv(f'../dataset/val_iter{it}.csv')\n",
    "train = pd.read_csv(f'../dataset/train_iter{it}.csv')\n",
    "train_and_test = pd.concat([train, test])\n",
    "train_and_test = train_and_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "idsf = '../dataset/ids/'\n",
    "fold1_train = open(idsf + 'fold1_train.txt').read().splitlines()\n",
    "fold2_train = open(idsf + 'fold2_train.txt').read().splitlines()\n",
    "fold3_train = open(idsf + 'fold3_train.txt').read().splitlines()\n",
    "fold4_train = open(idsf + 'fold4_train.txt').read().splitlines()\n",
    "fold5_train = open(idsf + 'fold5_train.txt').read().splitlines()\n",
    "\n",
    "fold1_test = open(idsf + 'fold1_test.txt').read().splitlines()\n",
    "fold2_test = open(idsf + 'fold2_test.txt').read().splitlines()\n",
    "fold3_test = open(idsf + 'fold3_test.txt').read().splitlines()\n",
    "fold4_test = open(idsf + 'fold4_test.txt').read().splitlines()\n",
    "fold5_test = open(idsf + 'fold5_test.txt').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_index(train_index, test_index):\n",
    "    train_idx, test_idx = [], []\n",
    "    for idx, row in train_and_test.iterrows():\n",
    "        dialog_id = row['video-id'].split('-')\n",
    "        if len(dialog_id) == 4:\n",
    "            dialog_id = '-'.join(dialog_id[:2])\n",
    "        else:\n",
    "            dialog_id = '-'.join(dialog_id[:3])\n",
    "        if dialog_id in train_index:\n",
    "            train_idx.append(idx)\n",
    "        else:\n",
    "            test_idx.append(idx)\n",
    "    return train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = [fold1_train, fold2_train, fold3_train, fold4_train, fold5_train]\n",
    "test_folds = [fold1_test, fold2_test, fold3_test, fold4_test, fold5_test]\n",
    "for i, (train_fold, test_fold) in enumerate(zip(train_folds, test_folds)):\n",
    "    train_idx, test_idx = get_train_test_index(train_fold, test_fold)\n",
    "    train = train_and_test.loc[train_idx]\n",
    "    test = train_and_test.loc[test_idx]\n",
    "    train.loc[:,['sent2', 'startphrase']] = train.loc[:,['startphrase','sent2']].values\n",
    "    test.loc[:,['sent2', 'startphrase']] = test.loc[:,['startphrase','sent2']].values\n",
    "    train.to_csv(f'../dataset/train_iter{it}_fold{i}.csv', index=False)\n",
    "    test.to_csv(f'../dataset/test_iter{it}_fold{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Generate dataset for only Questions, sent1 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    i = 4\n",
    "    train = pd.read_csv(f'/home/emrys/Github/csk_mcq/dataset/csk_mcq_22_01/train_iter10_fold{i}.csv')\n",
    "    test = pd.read_csv(f'/home/emrys/Github/csk_mcq/dataset/csk_mcq_22_01/test_iter10_fold{i}.csv')\n",
    "    train.loc[:, 'sent1'] = 'fake_sent1'\n",
    "    test.loc[:, 'sent1'] = 'fake_sent1'\n",
    "    train.to_csv(f'train_iter{it}_fold{i}_Q.csv', index=False)\n",
    "    test.to_csv(f'test_iter{it}_fold{i}_Q.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for only Ending. (not included in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train = pd.read_csv(f'/home/emrys/Github/csk_mcq/dataset/csk_mcq_22_01/train_iter10_fold{i}.csv')\n",
    "    test = pd.read_csv(f'/home/emrys/Github/csk_mcq/dataset/csk_mcq_22_01/test_iter10_fold{i}.csv')\n",
    "    train.loc[:, 'sent1'] = 'fake_sent1'\n",
    "    test.loc[:, 'sent1'] = 'fake_sent1'\n",
    "    train.loc[:, 'sent2'] = 'fake_sent2'\n",
    "    test.loc[:, 'sent2'] = 'fake_sent2'\n",
    "    train.to_csv(f'train_iter{it}_fold{i}_S.csv', index=False)\n",
    "    test.to_csv(f'test_iter{it}_fold{i}_S.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predictions (Not used in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load('/home/emrys/Github/transformers/examples/multiple-choice/predictions.npy')\n",
    "labels = np.load('/home/emrys/Github/transformers/examples/multiple-choice/label_ids.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.loc[predictions.argmax(1) != labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check negative options (Not used in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_options = json.load(open('negative_options.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for o in negative_options.values():\n",
    "    count += len(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.764682850430695"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count / len(negative_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
